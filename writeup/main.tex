\documentclass[a4paper,10pt]{article}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage[dvips]{hyperref}

\usepackage{listings}

\date{2010-08-23}

\lstset{breaklines=true, breakatwhitespace=true}
\newcommand{\prismmodel}[1]{
  \begin{quotation}
  \footnotesize
  \lstinputlisting{#1.sm}
  \end{quotation}
}
\newenvironment{prismprop}[0]{
  \begin{center}
  \begin{tabular}{c}
  \footnotesize
}{
  \end{tabular}
  \end{center}
}

\newtheorem*{thm}{Theorem}

\begin{document}

\section{Introduction}

Gossip (or epidemic) algorithms are an important tool in many peer-to-peer systems. Whilst there are a number of related algorithms that are referred to as gossip algorithms, \cite{pss} defines a core common to almost all of these: the peer sampling service (PSS). This service provides each node with a random selection of other nodes to gossip with. 

It is argued in \cite{formal_analysis} that one of the main points in favour of using gossip algorithms to build distributed systems is that their simplicity makes them amenable to formal analysis. Despite this, no formal analysis of a peer sampling service has been produced. Many empirical studies have been conducted \cite{pss, formal_analysis}, in particular \cite{pss} demonstrates that many of the assumptions commonly made about the behaviour of peer sampling services do not hold in practice. It has also been noted \cite{pss, arrg, how_robust, emergent_gossip} that the behaviour of gossip algorithms is often highly sensitive to small changes. Finally, many gossip algorithms have turned out to be surprisingly susceptible to malicious attack \cite{poison, poison_buddycast, poison_mosquito, poison_antimosquito}. For these reasons it would be beneficial to have a stronger theoretical foundation to work from.

In this dissertation we will present a peer sampling service for which we can make strong theoretical guarantees about its behaviour. Section 2 discusses related work and presents cyclon as a well-studied example of a gossip algorithm. Section 3 describes our general approach and introduces the necessary mathematics. Section 4 describes a centralised algorithm for providing independent, uniformly distributed peer samples. Section 5 discusses balancing the load of the centralised algorithm across multiple nodes. Section 6 introduces a fully distributed version of the previous algorithm. Section 7 investigates the algorithms resilience to random failure and churn. Section 8 presents a reference implementation and test results. Finally section 9 presents a conclusion and discusses future extensions.

\section{Background}

Gossip algorithms have been used for a wide variety of tasks, including overlay construction \cite{cyclon}, database replication \cite{database_replication}, failure detection \cite{failure_detection}, resource monitoring \cite{astrolabe}, distributed search/recommendation \cite{tribler}, reputation tracking \cite{reputation} and multicast messaging \cite{multicast}. 

Recently there has been a flurry of work aimed at rigourous analysis of these algorithms. These can be divided roughly into two groups: those studying the design and theoretical limitations of gossip algorithms operating on a fixed overlay, typically in the context of wireless devices \cite{sensorish1, sensorish2}, and those attempting to model the behaviour of gossip algorithms on dynamic, unstructured overlays \cite{generic_theory, mean_field, gossip_prism, correctness, random_scheduling}. We are interested here in the latter. Despite the amount of attention it has turned out to be very difficult to analyse such algorithms. 

\section{Approach}

We will argue that there are several properties of existing gossip algorithms that makes them fundamentally difficult to analyse.

The first of these is the use of fixed delays between actions at a single node. This causes complex scheduling of events across the network. In \cite{random_scheduling} the author begins by assuming that actions are scheduled randomly across the network. The resulting predictions are shown to be incorrect and the author is forced to move to a more complicated mathematical model. It is not difficult to arrange that events are scheduled uniformly at random across the network and this makes modelling much simpler.

The second is conflation of subsystems within an algorithm. Most gossip algorithms are presented as a single monolithic algorithm whereas it would be simpler to consider them as separate layers performing different functions. For example, in Cyclon and similar algorithms the network view and the peer sampling service are combined in a single data structure and both are updated simultaneously. It has been recognised \cite{pss, generic_theory} that the separation of these jobs is a natural simplification. Here we will concentrate entirely on the peer sampling service as a system providing a stream of random peer samples over time. Other subsystems, such as the network view, can be built as consumers of this stream.

The third, and somewhat vague, problem is that interactions between nodes are too complicated. We will show that the view shuffling used in most gossip algorithms is unnecessary - a peer sampling service can be produced without being aware of the network view at all.

Our inspiration for this algorithm comes from a natural example: radioactive decay. A group of radioactive atoms decays in a uniformly random order. That is, each atom has an equal probability to be the next atom to decay. This does not require any interaction between the atoms. It is a simple consequence of the exponential distribution of decay times.

The exponential distribution is governed by the probability distribution function $ f(x)=\lambda e ^ { - \lambda x } $. This distribution has a number of interesting properties. It is memoryless: given $ E \sim Exp(\lambda) $ we can prove that $ P(E>a+b \;|\; E>a) = P(E>b) $. Given two such variables $ E_1 \sim Exp(\lambda) $ and $ E_2 \sim Exp(\mu) $ we can prove that $ min(E_1,E_2) \sim Exp(\lambda + \mu) $ and $ P(E_1 < E_2) = \lambda / (\mu + \lambda) $. These properties can be naturally extended to more than two variables. 

We can see now why atoms decay randomly. Since they all have the same distribution they all have the same probability to be the first to decay. Once one atom has decayed the memoryless property means that we can 'reset' all the decay times.

For repeated actions we want to deal with a poisson process. A poisson process is a sequence of decay times for a given exponential distribution. More formally, given decay times $S_i \sim Exp(\lambda), \; i>0 $ the poisson process with rate $ \lambda $ is the sequence $ T_i = \sum_{k=0}^i S_k $. It is often useful to associate a set of random events $E_i$ with the process, so that the event $E_i$ is considered to have occurred at time $T_i$. 

Like the exponential distribution this process is memoryless - it looks the same starting from any point. It also has two properties which we will use repeatedly. The merging property governs interleaving two different poisson processes in a single process. The thinning property governs splitting a single poisson process into two independent processes.

\newtheorem*{merging}{Merging}
\begin{merging}
Given independent poisson processes $U=\{U_i | i \geq 0\}$ and $V=\{V_i | i \geq 0\}$ with rates $\lambda$ and $\mu$ respectively. WLOG let $U_0 < V_0$. Define the merging \;$T = merge(U,V)$\; of these processes recursively as follows: $T_0 = U_0$, $\{T_i \;|\; i>0\} = merge(\{U_i \;|\; i>0\}, V)$. Then $T$ is a poisson process with rate $\lambda + \mu$. Define $E_i$ to be $0$ if $T_i$ was drawn from $U$ and $1$ otherwise. Then the random variables $\{E_i \;|\; i \geq 0\}$ are independent and $E_i \sim Bin(\lambda / (\lambda + \mu))$. 
\end{merging}
 
\newtheorem*{thinning}{Thinning}
\begin{thinning}
Given a poisson process $T$ with rate $\lambda$ and a set $E_i$ of independent random binary variables $E_i ~ Bin(p)$. Define $U = \{T_i \;|\; E_i = 0, \; i \geq 0\}$ and $V = \{T_i \;|\; E_i = 1, \; i \geq 0\}$. Then $U$ and $V$ are independent poisson process with rates $\lambda p$ and $\lambda (1-p)$ respectively.
\end{thinning}

Both the merging and thinning properties can be extended in the natural way to deal with more than two poisson processes.

\section{Implementing a uniform PSS}

Pick some root node $R$, whose address is known to all other nodes. Every other node behaves as an independent poisson process sending messages to the root node at a rate $\lambda$. Let $R_i$ be the sender of the $i$th message to arrive at the root node. By the merging property the sequence $R_i$ forms a possion process of rate $n \lambda$ and each $R_i$ is an independent uniform random choice from the set of all nodes.

On receiving each message $R_i$, the root node sends the address of $R_{i-1}$ to $R_i$. Let $N_i$ be the $i$th message to arrive at node $N$ from the root $R$. Let $T^N_i$ be the index in $R$ of this message ie

\begin{gather*}
S_{T^N_i} = N \\
S_{T^N_i-1} = N_i \\
\end{gather*}

For each node $N$ the sequence of addresses $N_i$ will form the peer samples for $N$.

\begin{thm}For each node $N$ the peer samples $N_i$ are independent\end{thm}

\begin{proof}
For any node $M$
\begin{align*}
& \!\!\! P(N_i = M |\; N_0=n_0, ... ,N_{i-1} = n_{i-1}) \\
& = P(S_{T^N_i-1} = M |\; S_{T^N_0-1} = n_0, ... ,S_{T^N_{i-1}-1} = n_{i-1}) \\
& = P(S_{T_i-1} = M) \text{ since $S_i$ are independent} \\
\end{align*}
\end{proof}

\begin{thm}For each node N the peer samples $N_i, \; i>0$ are uniformly distributed over the set of all nodes (note that $N_0$ is not uniformly distributed since it can only be equal to $N$ when $S_0 = S_1 = N$)\end{thm}

\begin{proof}
Consider the peer sampling $N_i$. Define $K = T^N_i - T^N_{i-1}$.

\begin{align*}
P(&N_i = N) \\
& = \sum_{k>0} P(K=k) P(S_{T^N_i-1}=N | K=k) \\
& = P(K=1) P(S_{T^N_i-1}=N | K=1) \text{ since $\forall k>1 \;\; P(S_{T^N_i-1}=M | K=k) = 0$} \\
& = \frac{1}{n} \; 1 \\
\end{align*}

\noindent For $M \neq N$: \\
\begin{align*}
P(&N_i = M) \\
& = P(S_{T_i-1} = M \\
& = \sum_{k>0} P(K=k) P(S_{T^N_i-1}=M | K=k) \\
& = \sum_{k>1} P(K=k) P(S_{T^N_i-1}=M | K=k) \text{ since $P(S_{T^N_i-1}=M | K=1) = 0$} \\
& = \sum_{k>1} P(K=k) P(S_{T^N_i-1}=M | K=k, S_{T^N_i-1} /= N) \text{ since $T^N_{i-1} = T^N_i - K < T^N_i-1 < T^N_i$} \\
& = \sum_{k>1} P(K=k) (1 / n-1) \text{ since $S_i$ are independent} \\
& = \frac{1}{n-1} \sum_{k>1} P(K=k) \\
& = \frac{1}{n-1} P(K>1) \\
& = \frac{1}{n-1} (1 - \frac{1}{n}) \\
& = \frac{1}{n} \\
\end{align*}

\end{proof}

\begin{thm}The sampling times $T^N_i$ form a poisson process with rate $\lambda$, starting from time $\lambda$?\end{thm}

proof: thinning property? !!!

The system as a whole forms a continuous-time markov chain. Using the PRISM model checker we can provide an explicit and unambigous description of this algorithm and verify the desired properties.

\prismmodel{ctmc_single}

For the sake of clarity, in this section we will only present example models and describe the properties which need to be tested. More complete details on model generation and the methods for testing various properties are described in Appendix A.

In the above model the variable 'root' represents the last node to send a message to the root node and the variables 'node0', 'node1' and 'node2' represent the last peer sampling at their respective nodes.

Suppose we wanted to test the claim that each $N_i$ is uniformly distributed for $i>0$. This turns out to be very difficult in this model. We could test $P(N_i = M)$ for any fixed point in time and check that the value is the same for each node $M$:

\begin{prismprop}
\begin{lstlisting}
P=? [ F[T,T] node0=0 ]
P=? [ F[T,T] node0=1 ]
P=? [ F[T,T] node0=2 ]
\end{lstlisting}
\end{prismprop}

Or we could test the same thing in the steady state:

\begin{prismprop}
\begin{lstlisting}
S=? [ node0=0 ]
S=? [ node0=1 ]
S=? [ node0=2 ]
\end{lstlisting}
\end{prismprop}

Whilst these are nice properties to have neither of them are quite the same as saying that $N_i$ are uniformly distributed. The problem is that each $N_i$ is associated with a transition whereas the properties above are associated with points in time. It is awkward to specify PRISM properties relating to transitions when using a CTMC model. One workaround is to use transition rewards but this brings its own problems. Transition rewards have no meaning in the steady state and cannot be calculated by the PRISM simulator which makes working with large models impossible.

A simpler solution is to ignore transition times altogether and work with the jump process - the deterministic-time markov chain corresponding to the transitions of the CTMC. The translation is straightforward:

\prismmodel{dtmc_single}

Notice that the choice of which node will next send a message to the root has been made explicit. Also the 'Past' module has been added which tracks the last value of each variable. This makes it easy to identify peer samples. For example, if next\_past is 0 then node0 is a new peer sampling, replacing the previous sampling node0\_past.

Now we can directly test that the distribution of peer samples at a given node is uniform in the steady state. The following must all be equal:

\begin{prismprop}
\begin{lstlisting}
S=? [ next_past=0 & node0=0 ]
S=? [ next_past=0 & node0=1 ]
S=? [ next_past=0 & node0=2 ]
\end{lstlisting}
\end{prismprop}

Similarly we can test that each peer sampling is independent of the previous peer sampling by checking that the following are equal:

\begin{prismprop}
\begin{lstlisting}
S=? [ next_past=0 & node0=0 & node_past=0 ]
S=? [ next_past=0 & node0=0 & node_past=1 ]
S=? [ next_past=0 & node0=0 & node_past=2 ]

S=? [ next_past=0 & node0=0 & node_past=0 ]
S=? [ next_past=0 & node0=1 & node_past=1 ]
S=? [ next_past=0 & node0=2 & node_past=2 ]

S=? [ next_past=0 & node0=0 & node_past=0 ]
S=? [ next_past=0 & node0=1 & node_past=1 ]
S=? [ next_past=0 & node0=2 & node_past=2 ]
\end{lstlisting}
\end{prismprop}

For good measure, we can check in both models that the latest peer samples at each node are pairwise independent by testing:

\begin{prismprop}
\begin{lstlisting}
S=? [ node0=0 & node1=0 ]
S=? [ node0=0 & node1=1 ]
S=? [ node0=0 & node1=2 ]

S=? [ node0=1 & node1=0 ]
S=? [ node0=1 & node1=1 ]
S=? [ node0=1 & node1=2 ]

S=? [ node0=2 & node1=0 ]
S=? [ node0=2 & node1=1 ]
S=? [ node0=2 & node1=2 ]
\end{lstlisting}
\end{prismprop}

This algorithm is easy to reason about and provides strong guarantees about the distribution of peer samples. It is also simple enough that we can model it directly in PRISM and verify a subset of the properties that were proved analytically. Unfortunately it introduces a single point of failure at the root node which must sustain a load proportional to the number of nodes in the network. This is obviously unacceptable for a large network. The next sections will describe how to remove this point of failure.

\section{Spreading the load}

A simple aproach to improving the previous algorithm is to use more than one root node. We can replace the single root node with a set of root nodes $R_0 .. R_k$ which are known to every node. Each of the new root nodes behave identically to the root node in the previous algorithm. For each message the sending node chooses a root node uniformly at random to contact. 

Intuitively, this has the same steady-state properties as the previous algorithm. The argument is as follows: By the thinning property we can consider the messages arriving at each root node to be independent poisson processes each with rate $n \lambda / k$. Then each node receives $k$ independent peer sampling processes (one from each root node) each with rate $\lambda / k$ and with peer samples which are independent and uniformly distributed as proved for the previous algorithm. By the merging property we can combine these peer sampling processes to form a single process again with the same properties. 

One change we should be careful to note is that in the previous algorithm the first peer sampling $N_0$ was not uniformly distributed. Since we now have $k$ of these processes being interleaved we can't guarantee when the first peer sampling from each node will occur. We can guarantee that in the worst case each node has to send two messages to each root node before all future peer samples at that node will be uniformly distributed (the first message might be the first message at that node in which case it will not generate a peer sampling).

Define $I_N$ to be the point at which at least two messages have been sent from $N$ to each peer node.

thm: $E(I_N) <=$ 

Define $J_N$ to be the point at which at least one message has been sent from $N$ to each peer node.

$$
P(J_N = j)
= !!! can afford not to prove this
$$
 
We can model this new algorithm in PRISM:

\prismmodel{ctmc_multiple}

Again many properties are easier to prove using the jump process.

\prismmodel{dtmc_multiple}

We can reuse all the properties tested for the original algorithm without out change and, as expected, all of them still hold.

\section{Inside out}

This alleviates some of the load on the root and makes the network more resilient but we still have a predetermined set of root nodes to which we cannot add or remove nodes. Examining the proof above, the only reason that the set of root nodes needs to be known is in order to make a random sampling from it. These samples are required to be uniformly distributed and independent. What we need is a peer sampling service for the root nodes.

The solution is to feed the output of the algorithm back in. Each node can also act as a root node. The system is bootstrapped from a known set of root nodes so that each node has generated at least one peer sampling. Then these independent uniform peer samples are used to choose which node to contact next to receive a new peer sampling. !!! importantly, the output of the root nodes in independent of the root input so the peer samples continue to be suitable for use as independent choices of root node.

This seems intuitively correct but there are a number of subtle implementation details that have been glossed over. For example, will uniform sampling still hold if some nodes start acting as root nodes before others have bootstrapped? Rather than go into detail we will simply use the PRISM model as the !!! exact specifier majiggy !!!

\prismmodel{ctmc_broken}

Again, it is useful to work with the jump process as well.

\prismmodel{dtmc_broken}

!!! find, for both the CTMC and the DTMC:

\begin{prismprop}
\begin{lstlisting}
S=? [ node0=0 ]  0.31186...
S=? [ node0=1 ]  0.34407...
S=? [ node0=2 ]  0.34407...
\end{lstlisting}
\end{prismprop}

This is well outside the bounds of numerical error. What went wrong? 

!!! something about reachable states

\begin{prismprop}
\begin{lstlisting}
SCCs: 5, BSCCs: 5, non-BSCC states: 0
BSCC sizes: 1:683 2:15 3:15 4:15 5:1
\end{lstlisting}
\end{prismprop}

!!! stuff. 

This is reminiscent of the random surfer problem !!!cite, in which a person surfs the web by randomly following hyperlinks. The resulting markov chain has the same structure as the underlying hyperlink graph. If the hyperlink graph is not connected or if it has periodic states then the resulting markov chain will not have a well defined steady state. The solution is to add an arbitrarily small probability that the user will jump to page selected uniformly at random. This transition unifies unconnected states and removes periodicity. Our problem can be fixed in a similar manner by adding an arbitrarily small probability of contacting one of the known root nodes.

\prismmodel{ctmc_full}

Now all the states are reachable from the initial state.

\begin{prismprop}
\begin{lstlisting}
SCCs: 1, BSCCs: 1, non-BSCC states: 0
BSCC sizes: 1:729
\end{lstlisting}
\end{prismprop}

Translating to the jump process is still straightforward.

\prismmodel{dtmc_full}

!!! maybe talk about time before uniform distribution

Reassuringly, all of our tests now pass and we have a simple, distributed peer sampling service with strong guarantees on the distribution of peer samples. 

\section{Dealing with failure}

failure - prob p of message failure. 2 messages per exchange. short timeout for reply. if 1st fails, talk to root instead. mention caching from view as an alternate strategy. if second fails end up messaging the same node twice. should be a big deal

churn - model as low rate of switching on and off. messaging to an off node fails. node0 does not turn off, being a root. turn back on with last node we remember (larger state space) or just talking to root (smaller state space, can also model new joining vs on/off 

\section{Reference implementation}

\section{Conclusion}

We presented a gossip algorithm which is simple, easy to implement and resilient to random failure and churn. The peer sampling service produced by this algorithm generates independent, uniformly distributed peer samples when in the steady state. This analysis is backed up both by formal verification using the PRISM model checker and by experimental results. As far as the author is aware this is the first gossip algorithm to make \em any \em guarantees whatsoever about the behaviour of its peer sampling service.

This algorithm is suitable for use with trusted peers, for example within a data centre. Without further work it is not suitable for use with untrusted peers as it is susceptible to poisoning or other malicious behaviour. Future work could include identifying and managing malicious peers. In particular, it should be possible to rate-limit malicious peers without damaging the useful properties of the peer sampling service. This would go a long way towards preventing poisoning. Given the simplicity of this algorithm it may be even be possible to prove that specific classes of attacks will not work.

In addition, the use of frequent small messages to lots of different peers makes it difficult to perform NAT negotiation efficiently. Large scale uses of gossip algorithms have often found that NAT negotiation is essential - reliance on UPnP or similar is ineffective. One solution might be to reuse the root nodes as ICE servers for nodes which are known to be unable to open ports to the outside world.

\section{Appendix A: Model checking methods}

\section{Appendix B: Reference implementation}

\nocite{*}

\bibliographystyle{plain}
\bibliography{gossip}

\end{document}
