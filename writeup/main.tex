\documentclass[a4paper,10pt]{article}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage[dvips]{hyperref}

\usepackage{listings}

\date{2010-08-23}

\lstset{breaklines=true, breakatwhitespace=true}
\newcommand{\prismmodel}[1]{
  \begin{quotation}
  \footnotesize
  \lstinputlisting{#1.sm}
  \end{quotation}
}
\newenvironment{prismprop}[0]{
  \begin{center}
  \begin{tabular}{c}
  \footnotesize
}{
  \end{tabular}
  \end{center}
}

\newtheorem*{thm}{Theorem}

\begin{document}

\section{Introduction}

Gossip algorithms are an important tool in many peer-to-peer systems. Whilst there are a number of related algorithms that are referred to as gossip algorithms, \cite{pss} defines a core common to almost all of these: the peer sampling service (PSS). This service provides each node with a random selection of other nodes to gossip with. 

\cite{formal_analysis} argues that one of the main points in favour of using gossip algorithms to build distributed systems is that their simplicity makes them amenable to formal analysis. Despite this, no formal analysis of a peer sampling service has been produced. Many empirical studies have been conducted \cite{pss, formal_analysis}, in particular \cite{pss} demonstrates that many of the assumptions commonly made about the behaviour of the peer sampling service do not hold in practice. It has also been noted \cite{pss, arrg} that the behaviour of gossip algorithms is often highly sensitive to small changes. Finally, many gossip algorithms have turned out to be surprisingly susceptible to malicious attack \cite{poison, poison_buddycast, poison_buddycast2, poison_mosquito, poison_antimosquito}. For these reasons it would be beneficial to have a stronger theoretical foundation to work from.

In this dissertation we will present a peer sampling service for which we can make strong theoretical guarantees about its behaviour. Section 2 discusses related work and presents cyclon as a well-studied example of a gossip algorithm. Section 3 describes our general approach and introduces the necessary mathematics. Section 4 describes a centralised algorithm for providing independent, uniformly distributed peer samples. Section 5 discusses balancing the load of the centralised algorithm across multiple nodes. Section 6 introduces a fully distributed version of the previous algorithm. Section 7 investigates the algorithms resilience to random failure and churn. Section 8 presents a reference implementation and test results. Finally section 9 presents a conclusion and discusses future extensions.

\section{Related work}

what is a gossip algorithm?

list examples of gossip algorithms in use

database repli-
cation [26], but more recently also for failure detection [70],
and resource monitoring [69]


examples of partial models, copy to cite in intro

examine cyclon \cite{cyclon} and look at examples of difficult analysis

\section{Inspiration}

define pss properly, compare to entropy pool in /dev/random

cyclon is conflating view and pss
cyclon use fixed time intervals so must account for interleaving

an example in nature: radioactive decay.

talk about exponential distribution, poisson process. state and reference thinning and merging properties

\section{Implementing a uniform PSS}

Pick some root node $R$, whose address is known to all other nodes. Every other node behaves as an independent poisson process sending messages to the root node at a rate $\lambda$. Let $R_i$ be the sender of the $i$th message to arrive at the root node. By the merging property the sequence $R_i$ forms a possion process of rate $n \lambda$ and each $R_i$ is an independent uniform random choice from the set of all nodes.

On receiving each message $R_i$, the root node sends the address of $R_{i-1}$ to $R_i$. Let $N_i$ be the $i$th message to arrive at node $N$ from the root $R$. Let $T^N_i$ be the index in $R$ of this message ie

\begin{gather*}
S_{T^N_i} = N \\
S_{T^N_i-1} = N_i \\
\end{gather*}

For each node $N$ the sequence of addresses $N_i$ will form the peer samples for $N$.

\begin{thm}For each node $N$ the peer samples $N_i$ are independent\end{thm}

\begin{proof}
For any node $M$
\begin{align*}
& \!\!\! P(N_i = M |\; N_0=n_0, ... ,N_{i-1} = n_{i-1}) \\
& = P(S_{T^N_i-1} = M |\; S_{T^N_0-1} = n_0, ... ,S_{T^N_{i-1}-1} = n_{i-1}) \\
& = P(S_{T_i-1} = M) \text{ since $S_i$ are independent} \\
\end{align*}
\end{proof}

\begin{thm}For each node N the peer samples $N_i, \; i>0$ are uniformly distributed over the set of all nodes (note that $N_0$ is not uniformly distributed since it can only be equal to $N$ when $S_0 = S_1 = N$)\end{thm}

\begin{proof}
Consider the peer sampling $N_i$. Define $K = T^N_i - T^N_{i-1}$.

\begin{align*}
P(&N_i = N) \\
& = \sum_{k>0} P(K=k) P(S_{T^N_i-1}=N | K=k) \\
& = P(K=1) P(S_{T^N_i-1}=N | K=1) \text{ since $\forall k>1 \;\; P(S_{T^N_i-1}=M | K=k) = 0$} \\
& = \frac{1}{n} \; 1 \\
\end{align*}

\noindent For $M \neq N$: \\
\begin{align*}
P(&N_i = M) \\
& = P(S_{T_i-1} = M \\
& = \sum_{k>0} P(K=k) P(S_{T^N_i-1}=M | K=k) \\
& = \sum_{k>1} P(K=k) P(S_{T^N_i-1}=M | K=k) \text{ since $P(S_{T^N_i-1}=M | K=1) = 0$} \\
& = \sum_{k>1} P(K=k) P(S_{T^N_i-1}=M | K=k, S_{T^N_i-1} /= N) \text{ since $T^N_{i-1} = T^N_i - K < T^N_i-1 < T^N_i$} \\
& = \sum_{k>1} P(K=k) (1 / n-1) \text{ since $S_i$ are independent} \\
& = \frac{1}{n-1} \sum_{k>1} P(K=k) \\
& = \frac{1}{n-1} P(K>1) \\
& = \frac{1}{n-1} (1 - \frac{1}{n}) \\
& = \frac{1}{n} \\
\end{align*}

\end{proof}

\begin{thm}The sampling times $T^N_i$ form a poisson process with rate $\lambda$, starting from time $\lambda$?\end{thm}

proof: thinning property? !!!

The system as a whole forms a continuous-time markov chain. Using the PRISM model checker we can provide an explicit and unambigous description of this algorithm and verify the desired properties.

\prismmodel{ctmc_single}

For the sake of clarity, in this section we will only present example models and describe the properties which need to be tested. More complete details on model generation and the methods for testing various properties are described in Appendix A.

In the above model the variable 'root' represents the last node to send a message to the root node and the variables 'node0', 'node1' and 'node2' represent the last peer sampling at their respective nodes.

Suppose we wanted to test the claim that each $N_i$ is uniformly distributed for $i>0$. This turns out to be very difficult in this model. We could test $P(N_i = M)$ for any fixed point in time and check that the value is the same for each node $M$:

\begin{prismprop}
\begin{lstlisting}
P=? [ F[T,T] node0=0 ]
P=? [ F[T,T] node0=1 ]
P=? [ F[T,T] node0=2 ]
\end{lstlisting}
\end{prismprop}

Or we could test the same thing in the steady state:

\begin{prismprop}
\begin{lstlisting}
S=? [ node0=0 ]
S=? [ node0=1 ]
S=? [ node0=2 ]
\end{lstlisting}
\end{prismprop}

Whilst these are nice properties to have neither of them are quite the same as saying that $N_i$ are uniformly distributed. The problem is that each $N_i$ is associated with a transition whereas the properties above are associated with points in time. It is awkward to specify PRISM properties relating to transitions when using a CTMC model. One workaround is to use transition rewards but this brings its own problems. Transition rewards have no meaning in the steady state and cannot be calculated by the PRISM simulator which makes working with large models impossible.

A simpler solution is to ignore transition times altogether and work with the jump process - the deterministic-time markov chain corresponding to the transitions of the CTMC. The translation is straightforward:

\prismmodel{dtmc_single}

Notice that the choice of which node will next send a message to the root has been made explicit. Also the 'Past' module has been added which tracks the last value of each variable. This makes it easy to identify peer samples. For example, if next\_past is 0 then node0 is a new peer sampling, replacing the previous sampling node0\_past.

Now we can directly test that the distribution of peer samples at a given node is uniform in the steady state. The following must all be equal:

\begin{prismprop}
\begin{lstlisting}
S=? [ next_past=0 & node0=0 ]
S=? [ next_past=0 & node0=1 ]
S=? [ next_past=0 & node0=2 ]
\end{lstlisting}
\end{prismprop}

Similarly we can test that each peer sampling is independent of the previous peer sampling by checking that the following are equal:

\begin{prismprop}
\begin{lstlisting}
S=? [ next_past=0 & node0=0 & node_past=0 ]
S=? [ next_past=0 & node0=0 & node_past=1 ]
S=? [ next_past=0 & node0=0 & node_past=2 ]

S=? [ next_past=0 & node0=0 & node_past=0 ]
S=? [ next_past=0 & node0=1 & node_past=1 ]
S=? [ next_past=0 & node0=2 & node_past=2 ]

S=? [ next_past=0 & node0=0 & node_past=0 ]
S=? [ next_past=0 & node0=1 & node_past=1 ]
S=? [ next_past=0 & node0=2 & node_past=2 ]
\end{lstlisting}
\end{prismprop}

For good measure, we can check in both models that the latest peer samples at each node are pairwise independent by testing:

\begin{prismprop}
\begin{lstlisting}
S=? [ node0=0 & node1=0 ]
S=? [ node0=0 & node1=1 ]
S=? [ node0=0 & node1=2 ]

S=? [ node0=1 & node1=0 ]
S=? [ node0=1 & node1=1 ]
S=? [ node0=1 & node1=2 ]

S=? [ node0=2 & node1=0 ]
S=? [ node0=2 & node1=1 ]
S=? [ node0=2 & node1=2 ]
\end{lstlisting}
\end{prismprop}

This algorithm is easy to reason about and provides strong guarantees about the distribution of peer samples. It is also simple enough that we can model it directly in PRISM and verify a subset of the properties that were proved analytically. Unfortunately it introduces a single point of failure at the root node which must sustain a load proportional to the number of nodes in the network. This is obviously unacceptable for a large network. The next sections will describe how to remove this point of failure.

\section{Spreading the load}

A simple aproach to improving the previous algorithm is to use more than one root node. We can replace the single root node with a set of root nodes $R_0 .. R_k$ which are known to every node. Each of the new root nodes behave identically to the root node in the previous algorithm. For each message the sending node chooses a root node uniformly at random to contact. 

Intuitively, this has the same steady-state properties as the previous algorithm. The argument is as follows: By the thinning property we can consider the messages arriving at each root node to be independent poisson processes each with rate $n \lambda / k$. Then each node receives $k$ independent peer sampling processes (one from each root node) each with rate $\lambda / k$ and with peer samples which are independent and uniformly distributed as proved for the previous algorithm. By the merging property we can combine these peer sampling processes to form a single process again with the same properties. 

One change we should be careful to note is that in the previous algorithm the first peer sampling $N_0$ was not uniformly distributed. Since we now have $k$ of these processes being interleaved we can't guarantee when the first peer sampling from each node will occur. We can guarantee that in the worst case each node has to send two messages to each root node before all future peer samples at that node will be uniformly distributed (the first message might be the first message at that node in which case it will not generate a peer sampling).

Define $I_N$ to be the point at which at least two messages have been sent from $N$ to each peer node.

thm: $E(I_N) <=$ 

Define $J_N$ to be the point at which at least one message has been sent from $N$ to each peer node.

$$
P(J_N = j)
= !!! can afford not to prove this
$$
 
We can model this new algorithm in PRISM:

\prismmodel{ctmc_multiple}

Again many properties are easier to prove using the jump process.

\prismmodel{dtmc_multiple}

We can reuse all the properties tested for the original algorithm without out change and, as expected, all of them still hold.

\section{Inside out}

This alleviates some of the load on the root and makes the network more resilient but we still have a predetermined set of root nodes to which we cannot add or remove nodes. Examining the proof above, the only reason that the set of root nodes needs to be known is in order to make a random sampling from it. These samples are required to be uniformly distributed and independent. What we need is a peer sampling service for the root nodes.

The solution is to feed the output of the algorithm back in. Each node can also act as a root node. The system is bootstrapped from a known set of root nodes so that each node has generated at least one peer sampling. Then these independent uniform peer samples are used to choose which node to contact next to receive a new peer sampling. !!! importantly, the output of the root nodes in independent of the root input so the peer samples continue to be suitable for use as independent choices of root node.

This seems intuitively correct but there are a number of subtle implementation details that have been glossed over. For example, will uniform sampling still hold if some nodes start acting as root nodes before others have bootstrapped? Rather than go into detail we will simply use the PRISM model as the !!! exact specifier majiggy !!!

\prismmodel{ctmc_broken}

Again, it is useful to work with the jump process as well.

\prismmodel{dtmc_broken}

!!! find, for both the CTMC and the DTMC:

\begin{prismprop}
\begin{lstlisting}
S=? [ node0=0 ]  0.31186...
S=? [ node0=1 ]  0.34407...
S=? [ node0=2 ]  0.34407...
\end{lstlisting}
\end{prismprop}

This is well outside the bounds of numerical error. What went wrong? 

!!! something about reachable states

\begin{prismprop}
\begin{lstlisting}
SCCs: 5, BSCCs: 5, non-BSCC states: 0
BSCC sizes: 1:683 2:15 3:15 4:15 5:1
\end{lstlisting}
\end{prismprop}

!!! stuff. 

This is reminiscent of the random surfer problem, in which a person surfs the web by randomly following hyperlinks. The resulting markov chain has the same structure as the underlying hyperlink graph. If the hyperlink graph is not connected or if it has periodic states then the resulting markov chain will not have a well defined steady state. The solution is to add an arbitrarily small probability that the user will jump to page selected uniformly at random. This transition unifies unconnected states and removes periodicity. Our problem can be fixed in a similar manner by adding a arbitrarily small probability of contacting one of the known root nodes.

\prismmodel{ctmc_full}

Now all the states are reachable from the initial state.

\begin{prismprop}
\begin{lstlisting}
SCCs: 1, BSCCs: 1, non-BSCC states: 0
BSCC sizes: 1:729
\end{lstlisting}
\end{prismprop}

Translating to the jump process is still straightforward.

\prismmodel{dtmc_full}

!!! maybe talk about time before uniform distribution

Reassuringly, all of our tests now pass and we have a simple, distributed peer sampling service with strong guarantees on the distribution of peer samples. 

\section{Dealing with failure}

\section{Reference implementation}

\section{Conclusion}

We presented a gossip algorithm which is simple, easy to implement and resilient to random failure and churn. The peer sampling service produced by this algorithm generates independent, uniformly distributed peer samples when in the steady state. This analysis is backed up both by formal verification using the PRISM model checker and by experimental results. As far as the author is aware this is the first gossip algorithm to make \em any \em guarantees whatsoever about the behaviour of its peer sampling service.

This algorithm is suitable for use with trusted peers, for example within a data centre. Without further work it is not suitable for use with untrusted peers as it is susceptible to poisoning or other malicious behaviour. Future work could include identifying and managing malicious peers. In particular, it should be possible to rate-limit malicious peers without damaging the useful properties of the peer sampling service. This would go a long way towards preventing poisoning. Given the simplicity of this algorithm it may be even be possible to prove that specific classes of attacks will not work.

In addition, the use of frequent small messages to lots of different peers makes it difficult to perform NAT negotiation efficiently. Large scale uses of gossip algorithms have often found that NAT negotiation is essential - reliance on UPnP or similar is ineffective. One solution might be to reuse the root nodes as ICE servers for nodes which are known to be unable to open ports to the outside world.

\section{Appendix A: Model checking methods}

\section{Appendix B: Reference implementation}

\nocite{*}

\bibliographystyle{plain}
\bibliography{gossip}

\end{document}
